{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Parquet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"nyc-taxi-notebook\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.213:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>nyc-taxi-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11278da50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/07 22:48:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------+----------+-----------------+-----------------+--------+----------+------------------+------------------+-------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|       PU_zone|PU_borough|           PU_lon|           PU_lat| DO_zone|DO_borough|            DO_lon|            DO_lat|trip_id|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------+----------+-----------------+-----------------+--------+----------+------------------+------------------+-------+\n",
      "|       1| 2023-08-01 00:26:44|  2023-08-01 00:45:25|            1.0|          4.3|       1.0|                 N|         263|          90|           1|       21.9|  3.5|    0.5|      5.35|         0.0|                  1.0|       32.25|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      0|\n",
      "|       1| 2023-08-01 07:02:17|  2023-08-01 07:25:15|            1.0|          4.4|       1.0|                 N|         263|          90|           1|       24.7|  2.5|    0.5|       5.7|         0.0|                  1.0|        34.4|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      1|\n",
      "|       2| 2023-08-01 14:26:08|  2023-08-01 14:46:07|            1.0|         4.48|       1.0|                 N|         263|          90|           1|       24.0|  0.0|    0.5|       7.0|         0.0|                  1.0|        35.0|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      2|\n",
      "|       2| 2023-08-01 20:02:59|  2023-08-01 20:21:32|            1.0|         3.08|       1.0|                 N|         263|          90|           1|       18.4|  1.0|    0.5|       3.0|         0.0|                  1.0|        26.4|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      3|\n",
      "|       1| 2023-08-02 06:31:38|  2023-08-02 06:48:13|            1.0|          4.5|       1.0|                 N|         263|          90|           2|       21.2|  2.5|    0.5|       0.0|         0.0|                  1.0|        25.2|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      4|\n",
      "|       2| 2023-08-02 17:15:43|  2023-08-02 17:41:55|            1.0|         5.44|       1.0|                 N|         263|          90|           1|       28.9|  2.5|    0.5|      7.08|         0.0|                  1.0|       42.48|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      5|\n",
      "|       2| 2023-08-02 20:55:35|  2023-08-02 21:20:01|            1.0|         5.14|       1.0|                 N|         263|          90|           1|       28.2|  1.0|    0.5|      4.98|         0.0|                  1.0|       38.18|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      6|\n",
      "|       2| 2023-08-02 22:19:10|  2023-08-02 22:36:41|            2.0|         4.05|       1.0|                 N|         263|          90|           1|       21.2|  1.0|    0.5|      5.24|         0.0|                  1.0|       31.44|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      7|\n",
      "|       1| 2023-08-03 08:46:07|  2023-08-03 09:11:00|            1.0|          3.9|       1.0|                 N|         263|          90|           1|       24.0|  2.5|    0.5|       4.2|         0.0|                  1.0|        32.2|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      8|\n",
      "|       1| 2023-08-03 15:56:37|  2023-08-03 16:23:28|            1.0|          4.0|       1.0|                 N|         263|          90|           1|       22.6|  5.0|    0.5|       5.8|         0.0|                  1.0|        34.9|                 2.5|        0.0|Yorkville West| Manhattan|-73.9510100659439|40.77876573980772|Flatiron| Manhattan|-73.99697147759031|40.742278583312206|      9|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------+----------+-----------------+-----------------+--------+----------+------------------+------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data.parquet\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating our Fact & Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------+-----------+-------------+------------+--------------------+---------------------+-------------+------------+--------------+-------------+---------------------+\n",
      "|datetime_id|tpep_pickup_datetime|pick_up_hour|pick_up_day|pick_up_month|pick_up_year|pick_up_dayoftheweek|tpep_dropoff_datetime|drop_off_hour|drop_off_day|drop_off_month|drop_off_year|drop_off_dayoftheweek|\n",
      "+-----------+--------------------+------------+-----------+-------------+------------+--------------------+---------------------+-------------+------------+--------------+-------------+---------------------+\n",
      "|          0| 2009-01-01 04:36:53|           4|          1|            1|        2009|                   5|  2009-01-02 00:00:00|            0|           2|             1|         2009|                    6|\n",
      "|          1| 2009-01-01 05:56:45|           5|          1|            1|        2009|                   5|  2009-01-01 06:09:59|            6|           1|             1|         2009|                    5|\n",
      "|          2| 2009-01-01 00:04:41|           0|          1|            1|        2009|                   5|  2009-01-01 13:46:18|           13|           1|             1|         2009|                    5|\n",
      "|          3| 2009-01-01 20:07:44|          20|          1|            1|        2009|                   5|  2009-01-01 20:08:24|           20|           1|             1|         2009|                    5|\n",
      "|          4| 2023-08-01 00:26:44|           0|          1|            8|        2023|                   3|  2023-08-01 00:45:25|            0|           1|             8|         2023|                    3|\n",
      "|          5| 2023-08-01 07:02:17|           7|          1|            8|        2023|                   3|  2023-08-01 07:25:15|            7|           1|             8|         2023|                    3|\n",
      "|          6| 2023-08-01 14:26:08|          14|          1|            8|        2023|                   3|  2023-08-01 14:46:07|           14|           1|             8|         2023|                    3|\n",
      "|          7| 2023-08-01 20:02:59|          20|          1|            8|        2023|                   3|  2023-08-01 20:21:32|           20|           1|             8|         2023|                    3|\n",
      "|          8| 2023-08-01 01:01:21|           1|          1|            8|        2023|                   3|  2023-08-01 01:32:24|            1|           1|             8|         2023|                    3|\n",
      "|          9| 2023-08-01 01:05:11|           1|          1|            8|        2023|                   3|  2023-08-01 01:35:50|            1|           1|             8|         2023|                    3|\n",
      "+-----------+--------------------+------------+-----------+-------------+------------+--------------------+---------------------+-------------+------------+--------------+-------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Date Time Dimension\n",
    "datetime_dim = df.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n",
    "datetime_dim = (datetime_dim\n",
    "                .withColumn(\"pick_up_hour\",f.hour(\"tpep_pickup_datetime\"))\n",
    "                .withColumn(\"pick_up_day\",f.dayofmonth(\"tpep_pickup_datetime\"))\n",
    "                .withColumn(\"pick_up_month\",f.month(\"tpep_pickup_datetime\"))\n",
    "                .withColumn(\"pick_up_year\",f.year(\"tpep_pickup_datetime\"))\n",
    "                .withColumn(\"pick_up_dayoftheweek\",f.dayofweek(\"tpep_pickup_datetime\"))\n",
    "                .withColumn(\"drop_off_hour\",f.hour(\"tpep_dropoff_datetime\"))\n",
    "                .withColumn(\"drop_off_day\",f.dayofmonth(\"tpep_dropoff_datetime\"))\n",
    "                .withColumn(\"drop_off_month\",f.month(\"tpep_dropoff_datetime\"))\n",
    "                .withColumn(\"drop_off_year\",f.year(\"tpep_dropoff_datetime\"))\n",
    "                .withColumn(\"drop_off_dayoftheweek\",f.dayofweek(\"tpep_dropoff_datetime\")))\n",
    "datetime_dim = datetime_dim.sort(\"pick_up_day\", \"pick_up_month\", \"pick_up_year\")\n",
    "\n",
    "\n",
    "### Creating Index\n",
    "datetime_dim = datetime_dim.select(\"*\").withColumn(\"datetime_id\", monotonically_increasing_id())\n",
    "\n",
    "### Reordering Columns\n",
    "datetime_dim = datetime_dim.select(\"datetime_id\", \"tpep_pickup_datetime\", \"pick_up_hour\", \"pick_up_day\",\n",
    "                                   \"pick_up_month\", \"pick_up_year\", \"pick_up_dayoftheweek\",\n",
    "                                   \"tpep_dropoff_datetime\", \"drop_off_hour\", \"drop_off_day\",\n",
    "                                   \"drop_off_month\", \"drop_off_year\", \"drop_off_dayoftheweek\")\n",
    "\n",
    "datetime_dim.show(10)\n",
    "#datetime_dim.coalesce(1).write.format(\"parquet\").saveAsTable(\"tables/datetime_dim.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|passenger_count_id|passenger_count|\n",
      "+------------------+---------------+\n",
      "|                 0|           null|\n",
      "|                 1|            0.0|\n",
      "|                 2|            1.0|\n",
      "|                 3|            2.0|\n",
      "|                 4|            3.0|\n",
      "|                 5|            4.0|\n",
      "|                 6|            5.0|\n",
      "|                 7|            6.0|\n",
      "|                 8|            7.0|\n",
      "|                 9|            8.0|\n",
      "|                10|            9.0|\n",
      "+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Passenger Count Dimension\n",
    "passenger_count_dim = df.select(\"passenger_count\")\n",
    "passenger_count_dim = passenger_count_dim.dropDuplicates(subset = ['passenger_count']).sort(\"passenger_count\")\n",
    "\n",
    "# ### Creating Index                                     )\n",
    "passenger_count_dim = passenger_count_dim.select(\"*\").withColumn(\"passenger_count_id\", monotonically_increasing_id())\n",
    "\n",
    "# ### Reordering Columns\n",
    "passenger_count_dim = passenger_count_dim.select(\"passenger_count_id\", \"passenger_count\")\n",
    "passenger_count_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|trip_distance_id|trip_distance|\n",
      "+----------------+-------------+\n",
      "|               0|          0.0|\n",
      "|               1|         0.01|\n",
      "|               2|         0.02|\n",
      "|               3|         0.03|\n",
      "|               4|         0.04|\n",
      "|               5|         0.05|\n",
      "|               6|         0.06|\n",
      "|               7|         0.07|\n",
      "|               8|         0.08|\n",
      "|               9|         0.09|\n",
      "+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Trip Distance Dimension\n",
    "trip_distance_dim = df.select(\"trip_distance\")\n",
    "trip_distance_dim = trip_distance_dim.dropDuplicates(subset = ['trip_distance']).sort(\"trip_distance\")\n",
    "\n",
    "\n",
    "# ### Creating Index\n",
    "trip_distance_dim = trip_distance_dim.select(\"*\").withColumn(\"trip_distance_id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "### Reordering Columns\n",
    "trip_distance_dim = trip_distance_dim.select(\"trip_distance_id\", \"trip_distance\")\n",
    "trip_distance_dim.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+\n",
      "|ratecode_id|RatecodeID|      rate_code_name|\n",
      "+-----------+----------+--------------------+\n",
      "|          0|       1.0|       Standard rate|\n",
      "|          1|       2.0|                 JFK|\n",
      "|          2|       3.0|              Newark|\n",
      "|          3|       4.0|Nassau or Westche...|\n",
      "|          4|       5.0|     Negotiated fare|\n",
      "+-----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Rate Code Dimension\n",
    "\n",
    "### Assigning rate code mapping data as per the data dictionary\n",
    "rate_code_data = [\n",
    "    (1,\"Standard rate\"),\n",
    "    (2,\"JFK\"),\n",
    "    (3,\"Newark\"),\n",
    "    (4,\"Nassau or Westchester\"),\n",
    "    (5,\"Negotiated fare\"),\n",
    "    (6,\"Group ride\")\n",
    "]\n",
    "### Creating new dataframe from mapping data\n",
    "rate_code_type = spark.createDataFrame(data=rate_code_data, schema = [\"RatecodeID\",\"rate_code_name\"])\n",
    "### Investigating the schema of the columns \n",
    "# rate_code_type.printSchema() \n",
    "### Changing column schema to match expected datatype for the join\n",
    "rate_code_type = rate_code_type.select(f.expr(\"CAST(RatecodeID AS double) AS RatecodeID\"), \"rate_code_name\") \n",
    "\n",
    "### Creating Rate Code Dimension\n",
    "rate_code_dim = df.select(\"RatecodeID\")\n",
    "rate_code_dim = rate_code_dim.dropDuplicates(subset = ['RatecodeID']).sort(\"RatecodeID\")\n",
    "\n",
    "\n",
    "### Creating Index\n",
    "rate_code_dim = rate_code_dim.select(\"*\").withColumn(\"ratecode_id\", monotonically_increasing_id()-1) # The index started at 1 in this case, so a -1 was inserted\n",
    "\n",
    "### Joining dataframes on RatecodeID\n",
    "rate_code_dim = rate_code_dim.join(rate_code_type,\n",
    "                   [\"RatecodeID\"])\n",
    "\n",
    "### Reordering Columns\n",
    "rate_code_dim = rate_code_dim.select(\"ratecode_id\", \"RatecodeID\", \"rate_code_name\")\n",
    "rate_code_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+-------------+--------------------+\n",
      "|pickup_location_id|            PU_lon|            PU_lat|   PU_borough|             PU_zone|\n",
      "+------------------+------------------+------------------+-------------+--------------------+\n",
      "|                 0|-74.23353546082052| 40.52549110546785|Staten Island|Charleston/Totten...|\n",
      "|                 1|-74.15089028926955| 40.55186202510041|Staten Island|         Great Kills|\n",
      "|                 2|-74.18848459794721| 40.55265878064343|Staten Island|       Arden Heights|\n",
      "|                 3|-73.90691199328067|40.559134755628214|       Queens|Breezy Point/Fort...|\n",
      "|                 4|-74.12258304710812| 40.56199406259296|Staten Island|             Oakwood|\n",
      "|                 5|-74.10501884907058| 40.57176876885639|Staten Island|New Dorp/Midland ...|\n",
      "|                 6|-74.18642081572338| 40.57677255023222|Staten Island|     Freshkills Park|\n",
      "|                 7|-73.98794360684474|40.576961310705336|     Brooklyn|        Coney Island|\n",
      "|                 8|-73.84345437593673| 40.57798298905335|       Queens|       Rockaway Park|\n",
      "|                 9|-73.94362868028261| 40.58047335130059|     Brooklyn|     Manhattan Beach|\n",
      "+------------------+------------------+------------------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Pickup Location Dimension\n",
    "pickup_location_dim = df.select(\"PU_lon\", \"PU_lat\", \"PU_borough\",\"PU_zone\")\n",
    "pickup_location_dim = pickup_location_dim.dropDuplicates(subset = ['PU_lat']).sort(\"PU_lat\")\n",
    "\n",
    "### Creating Index\n",
    "pickup_location_dim = pickup_location_dim.select(\"*\").withColumn(\"pickup_location_id\", monotonically_increasing_id())\n",
    "\n",
    "### Reordering Columns\n",
    "pickup_location_dim = pickup_location_dim.select(\"pickup_location_id\", \"PU_lon\", \"PU_lat\", \"PU_borough\",\"PU_zone\")\n",
    "\n",
    "pickup_location_dim.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+-------------+--------------------+\n",
      "|dropoff_location_id|            DO_lon|            DO_lat|   DO_borough|             DO_zone|\n",
      "+-------------------+------------------+------------------+-------------+--------------------+\n",
      "|                  0|-74.23353546082052| 40.52549110546785|Staten Island|Charleston/Totten...|\n",
      "|                  1|-74.18767927741463|40.528685582160755|Staten Island|Eltingville/Annad...|\n",
      "|                  2|-74.20782577848769|40.540333107919224|Staten Island|   Rossville/Woodrow|\n",
      "|                  3|-74.15089028926955| 40.55186202510041|Staten Island|         Great Kills|\n",
      "|                  4|-74.18848459794721| 40.55265878064343|Staten Island|       Arden Heights|\n",
      "|                  5|-73.90691199328067|40.559134755628214|       Queens|Breezy Point/Fort...|\n",
      "|                  6|-74.12258304710812| 40.56199406259296|Staten Island|             Oakwood|\n",
      "|                  7|-74.10501884907058| 40.57176876885639|Staten Island|New Dorp/Midland ...|\n",
      "|                  8|-74.18642081572338| 40.57677255023222|Staten Island|     Freshkills Park|\n",
      "|                  9|-73.98794360684474|40.576961310705336|     Brooklyn|        Coney Island|\n",
      "+-------------------+------------------+------------------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Drop Off Location Dimension\n",
    "dropoff_location_dim = df.select(\"DO_lon\", \"DO_lat\", \"DO_borough\",\"DO_zone\")\n",
    "dropoff_location_dim = dropoff_location_dim.dropDuplicates(subset = ['DO_lat']).sort(\"DO_lat\")\n",
    "\n",
    "### Creating Index\n",
    "dropoff_location_dim = dropoff_location_dim.select(\"*\").withColumn(\"dropoff_location_id\", monotonically_increasing_id())\n",
    "\n",
    "### Reordering Columns\n",
    "dropoff_location_dim = dropoff_location_dim.select(\"dropoff_location_id\", \"DO_lon\", \"DO_lat\", \"DO_borough\",\"DO_zone\")\n",
    "\n",
    "dropoff_location_dim.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------------+\n",
      "|payment_type_id|payment_type|payment_type_name|\n",
      "+---------------+------------+-----------------+\n",
      "|              0|           1|      Credit card|\n",
      "|              1|           2|             Cash|\n",
      "|              2|           3|        No charge|\n",
      "|              3|           4|          Dispute|\n",
      "+---------------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Payment Type Dimension\n",
    "### Assigning rate code mapping data as per the data dictionary\n",
    "payment_type_data = [\n",
    "    (1,\"Credit card\"),\n",
    "    (2,\"Cash\"),\n",
    "    (3,\"No charge\"),\n",
    "    (4,\"Dispute\"),\n",
    "    (5,\"Unknown\"),\n",
    "    (6,\"Voided trip\")\n",
    "]\n",
    "### Creating new dataframe from mapping data\n",
    "payment_type_name = spark.createDataFrame(data=payment_type_data, schema = [\"payment_type\",\"payment_type_name\"])\n",
    "### Investigating the schema of the columns\n",
    "# payment_type_name.printSchema()\n",
    "\n",
    "### Creating Rate Code Dimension\n",
    "payment_type_dim = df.select(\"payment_type\")\n",
    "payment_type_dim = payment_type_dim.dropDuplicates(subset = ['payment_type']).sort(\"payment_type\")\n",
    "\n",
    "### Creating Index\n",
    "payment_type_dim = payment_type_dim.select(\"*\").withColumn(\"payment_type_id\", monotonically_increasing_id()-1)\n",
    "\n",
    "### Joining dataframes on RatecodeID\n",
    "payment_type_dim = payment_type_dim.join(payment_type_name,\n",
    "                   [\"payment_type\"])\n",
    "\n",
    "### Reordering Columns\n",
    "payment_type_dim = payment_type_dim.select(\"payment_type_id\", \"payment_type\", \"payment_type_name\")\n",
    "payment_type_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+------------------+----------------+-----------+------------------+------------------+-------------------+---------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|trip_id|VendorID|datetime_id|passenger_count_id|trip_distance_id|ratecode_id|store_and_fwd_flag|pickup_location_id|dropoff_location_id|payment_type_id|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+-------+--------+-----------+------------------+----------------+-----------+------------------+------------------+-------------------+---------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|      0|       1|          4|                 2|             430|          0|                 N|               189|                146|              0|       21.9|  3.5|    0.5|      5.35|         0.0|                  1.0|       32.25|\n",
      "|      1|       1|          5|                 2|             440|          0|                 N|               189|                146|              0|       24.7|  2.5|    0.5|       5.7|         0.0|                  1.0|        34.4|\n",
      "|      2|       2|      92903|                 2|             448|          0|                 N|               189|                146|              0|       24.0|  0.0|    0.5|       7.0|         0.0|                  1.0|        35.0|\n",
      "|      2|       2|      80309|                 2|             448|          0|                 N|               189|                146|              0|       24.0|  0.0|    0.5|       7.0|         0.0|                  1.0|        35.0|\n",
      "|      2|       2|      21745|                 2|             448|          0|                 N|               189|                146|              0|       24.0|  0.0|    0.5|       7.0|         0.0|                  1.0|        35.0|\n",
      "|      2|       2|      16987|                 2|             448|          0|                 N|               189|                146|              0|       24.0|  0.0|    0.5|       7.0|         0.0|                  1.0|        35.0|\n",
      "|      2|       2|          6|                 2|             448|          0|                 N|               189|                146|              0|       24.0|  0.0|    0.5|       7.0|         0.0|                  1.0|        35.0|\n",
      "|      3|       2|       8735|                 2|             308|          0|                 N|               189|                146|              0|       18.4|  1.0|    0.5|       3.0|         0.0|                  1.0|        26.4|\n",
      "|      3|       2|          7|                 2|             308|          0|                 N|               189|                146|              0|       18.4|  1.0|    0.5|       3.0|         0.0|                  1.0|        26.4|\n",
      "|      4|       1|      98910|                 2|             450|          0|                 N|               189|                146|              1|       21.2|  2.5|    0.5|       0.0|         0.0|                  1.0|        25.2|\n",
      "+-------+--------+-----------+------------------+----------------+-----------+------------------+------------------+-------------------+---------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Fact Table\n",
    "\n",
    "### Joining data from dimentions into dact table\n",
    "fact_table = df.join(f.broadcast(passenger_count_dim), df[\"passenger_count\"] == passenger_count_dim[\"passenger_count\"]) \\\n",
    "               .join(f.broadcast(trip_distance_dim), df[\"trip_distance\"] == trip_distance_dim[\"trip_distance\"]) \\\n",
    "               .join(f.broadcast(rate_code_dim), df[\"RatecodeID\"] == rate_code_dim[\"RatecodeID\"]) \\\n",
    "               .join(f.broadcast(pickup_location_dim), df[\"PU_lon\"] == pickup_location_dim[\"PU_lon\"]) \\\n",
    "               .join(f.broadcast(dropoff_location_dim), df[\"DO_lon\"] == dropoff_location_dim[\"DO_lon\"]) \\\n",
    "               .join(f.broadcast(datetime_dim), df[\"tpep_pickup_datetime\"] == datetime_dim[\"tpep_pickup_datetime\"]) \\\n",
    "               .join(f.broadcast(payment_type_dim), df[\"payment_type\"] == payment_type_dim[\"payment_type\"]) \\\n",
    "               .select(\"trip_id\",\"VendorID\", \"datetime_id\", \"passenger_count_id\", \"trip_distance_id\",\n",
    "                       \"ratecode_id\", \"store_and_fwd_flag\", \"pickup_location_id\", \"dropoff_location_id\",\n",
    "                       \"payment_type_id\", \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\",\n",
    "                       \"tolls_amount\", \"improvement_surcharge\", \"total_amount\")\n",
    "\n",
    "fact_table.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the session \n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data to filepath for upload\n",
    "The below two cells are the builiding blocks to the function which saves each pyspark dataframe as a singular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_table.coalesce(1).write.parquet(\"temp/fact_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = os.listdir(\"temp/fact_table.parquet\")\n",
    "    \n",
    "# name = \"\"\n",
    "# for filename in filenames:\n",
    "#     if filename.endswith(\".snappy.parquet\"):\n",
    "#         name += filename\n",
    "\n",
    "# if not os.path.exists('tables'):\n",
    "#    os.makedirs('tables')\n",
    "\n",
    "# path1 = f\"temp/fact_table.parquet/{name}\"\n",
    "# path2 = \"tables/fact_table.snappy.parquet\"\n",
    "\n",
    "# shutil.move(path1, path2)\n",
    "# shutil.rmtree(\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the logic has been sorted, proceeding with developing the function & saving each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_table(table, str):\n",
    "    if os.path.exists(\"temp\"):\n",
    "        shutil.rmtree(\"temp\")\n",
    "\n",
    "    # Pyspark syntax to save dataframe within designated filepath\n",
    "    table.coalesce(1).write.option(\"compression\", \"uncompressed\").parquet(\"temp/table.parquet\")\n",
    "\n",
    "    # Obtaining the name of our generated parquet file\n",
    "    filenames = os.listdir(\"temp/table.parquet\")\n",
    "    name = \"\"\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\"parquet\"):\n",
    "            name += filename\n",
    "\n",
    "    # Preparing a desired filepath to move our parquet files to\n",
    "    if not os.path.exists(\"tables\"):\n",
    "        os.makedirs(\"tables\")\n",
    "\n",
    "    path1 = f\"temp/table.parquet/{name}\"\n",
    "    path2 = f\"tables/{str}.parquet\"\n",
    "    \n",
    "    # Moving our parquet file to a desired filepath & renaming it\n",
    "    shutil.move(path1, path2)\n",
    "    \n",
    "    # Deleting the generated filepath from pyspark\n",
    "    shutil.rmtree(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our tables!\n",
    "save_table(passenger_count_dim, \"passenger_count_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table(trip_distance_dim, \"trip_distance_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table(rate_code_dim, \"rate_code_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table(pickup_location_dim, \"pickup_location_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table(dropoff_location_dim, \"dropoff_location_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "save_table(datetime_dim, \"datetime_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table(payment_type_dim, \"payment_type_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "save_table(fact_table, \"fact_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
